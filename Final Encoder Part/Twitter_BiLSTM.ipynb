{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "WARNING:tensorflow:From C:\\Users\\Arshad\\Anaconda3\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Arshad\\Anaconda3\\lib\\site-packages\\tflearn\\helpers\\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Arshad\\Anaconda3\\lib\\site-packages\\tflearn\\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Arshad\\Anaconda3\\lib\\site-packages\\tflearn\\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Arshad\\Anaconda3\\lib\\site-packages\\tflearn\\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Arshad\\Anaconda3\\lib\\site-packages\\tflearn\\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow \n",
    "from models import get_model\n",
    "import argparse\n",
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import preprocessor as p\n",
    "from collections import Counter\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from tensorflow.contrib import learn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from scipy import stats\n",
    "import tflearn\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global NUM_CLASSES\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading The file from CSV\n",
    "def load_data(filename):\n",
    "    data = pd.read_csv(filename, header = 0)\n",
    "\n",
    "    x = data['text'].to_list()\n",
    "    y = data['HS'].to_list()\n",
    "    return x,y\n",
    "\n",
    "# Entering the file details to get the data\n",
    "def get_filename(dataset):\n",
    "    global NUM_CLASSES, HASH_REMOVE\n",
    "    filename = \"data\\hateval2019_en_train.csv\"\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluation of the model based on F1 score\n",
    "def evaluate_model(model, testX, testY):\n",
    "    temp = model.predict(testX)\n",
    "    y_pred  = np.argmax(temp, 1)\n",
    "    y_true = np.argmax(testY, 1)\n",
    "    precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "    recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "    f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "    print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(data, x_text, labels):\n",
    "    \n",
    "    NUM_CLASSES = 2\n",
    "    # Splitting the dataset\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split( x_text, labels, random_state=42, test_size=0.10)\n",
    "    \n",
    "    \n",
    "    # Adding a padding to make all the text uniform\n",
    "    post_length = np.array([len(x.split(\" \")) for x in x_text])\n",
    "    if(data != \"twitter\"):\n",
    "        max_document_length = int(np.percentile(post_length, 95))\n",
    "    else:\n",
    "        max_document_length = max(post_length)\n",
    "    print(\"Document length : \" + str(max_document_length))\n",
    "    \n",
    "    \n",
    "    # Mapping words with vocubulary\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, MAX_FEATURES)\n",
    "    vocab_processor = vocab_processor.fit(x_text)\n",
    "    \n",
    "    # Mapping it with vocabulary\n",
    "    trainX = np.array(list(vocab_processor.transform(X_train)))\n",
    "    testX = np.array(list(vocab_processor.transform(X_test)))\n",
    "    \n",
    "    # Converting the variables into Array\n",
    "    trainY = np.asarray(Y_train)\n",
    "    testY = np.asarray(Y_test)\n",
    "    \n",
    "    # Padding the sequences\n",
    "    trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
    "    testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
    "\n",
    "\n",
    "    # Making a categorical conversion\n",
    "    trainY = to_categorical(trainY, nb_classes=NUM_CLASSES)\n",
    "    testY = to_categorical(testY, nb_classes=NUM_CLASSES)\n",
    "    \n",
    "    \n",
    "    # Making the final dictionary\n",
    "    data_dict = {\n",
    "        \"data\": data,\n",
    "        \"trainX\" : trainX,\n",
    "        \"trainY\" : trainY,\n",
    "        \"testX\" : testX,\n",
    "        \"testY\" : testY,\n",
    "        \"vocab_processor\" : vocab_processor\n",
    "    }\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_data(data_dict):\n",
    "    return data_dict[\"data\"], data_dict[\"trainX\"], data_dict[\"trainY\"], data_dict[\"testX\"], data_dict[\"testY\"], data_dict[\"vocab_processor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_weights(model, weights=None):\n",
    "    \"\"\"Randomly permute the weights in `model`, or the given `weights`.\n",
    "    This is a fast approximation of re-initializing the weights of a model.\n",
    "    Assumes weights are distributed independently of the dimensions of the weight tensors\n",
    "      (i.e., the weights have the same distribution along each dimension).\n",
    "    :param Model model: Modify the weights of the given model.\n",
    "    :param list(ndarray) weights: The model's weights will be replaced by a random permutation of these weights.\n",
    "      If `None`, permute the model's current weights.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = model.get_weights()\n",
    "    weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "    # Faster, but less random: only permutes along the first dimension\n",
    "    # weights = [np.random.permutation(w) for w in weights]\n",
    "    model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data_dict, model_type, vector_type, embed_size, dump_embeddings=False):\n",
    "\n",
    "    data, trainX, trainY, testX, testY, vocab_processor = return_data(data_dict)\n",
    "\n",
    "    vocab_size = len(vocab_processor.vocabulary_)\n",
    "    print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
    "    vocab = vocab_processor.vocabulary_._mapping\n",
    "    \n",
    "    # Training the model\n",
    "    print(\"Running Model: \" + model_type + \" with word vector initiliazed with \" + vector_type + \" word vectors.\")\n",
    "    model = get_model(model_type, trainX.shape[1], vocab_size, embed_size, NUM_CLASSES, LEARN_RATE)\n",
    "    model.summary()\n",
    "    initial_weights = model.get_weights()\n",
    "    shuffle_weights(model, initial_weights)\n",
    "    print(\"Loading start\")\n",
    "    model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "    # Storing the trained Model\n",
    "    model.save(\"Saved_model_Twitter_Hate_speech.h5\")\n",
    "        \n",
    "    return  evaluate_model(model, trainX, trainY), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_scores(precision_scores, recall_scores, f1_scores):\n",
    "    for i in range(NUM_CLASSES):\n",
    "        print(\"\\nPrecision Class %d (avg): %0.3f (+/- %0.3f)\" % (i, precision_scores[:, i].mean(), precision_scores[:, i].std() * 2))\n",
    "        print( \"\\nRecall Class %d (avg): %0.3f (+/- %0.3f)\" % (i, recall_scores[:, i].mean(), recall_scores[:, i].std() * 2))\n",
    "        print( \"\\nF1 score Class %d (avg): %0.3f (+/- %0.3f)\" % (i, f1_scores[:, i].mean(), f1_scores[:, i].std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(data, oversampling_rate):\n",
    "    \n",
    "    x_text, labels = load_data(get_filename(data)) \n",
    "    filter_data = []\n",
    "    for text in x_text:\n",
    "        filter_data.append(\"\".join(l for l in text if l not in string.punctuation)) \n",
    "    return x_text, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All the hyperparameters and model selection\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 128\n",
    "MAX_FEATURES = 2\n",
    "NUM_CLASSES = 1\n",
    "DROPOUT = 0.25\n",
    "LEARN_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(data, oversampling_rate, model_type, vector_type, embed_size):    \n",
    "    x_text, labels = get_data(data, oversampling_rate)\n",
    "    data_dict = get_train_test(data,  x_text, labels)\n",
    "    accuracy, model = train(data_dict, model_type, vector_type, embed_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document length : 63\n",
      "WARNING:tensorflow:From <ipython-input-5-0291f7a2be96>:18: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\Users\\Arshad\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\Users\\Arshad\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "Vocabulary Size: 7038\n",
      "Running Model: blstm with word vector initiliazed with random word vectors.\n",
      "WARNING:tensorflow:From C:\\Users\\Arshad\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 63, 200)           1407600   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 63, 200)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 63, 400)           641600    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 63, 400)           961600    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 802       \n",
      "=================================================================\n",
      "Total params: 3,973,202\n",
      "Trainable params: 3,973,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 63, 200)           1407600   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 63, 200)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 63, 400)           641600    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 63, 400)           961600    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 802       \n",
      "=================================================================\n",
      "Total params: 3,973,202\n",
      "Trainable params: 3,973,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Loading start\n",
      "WARNING:tensorflow:From C:\\Users\\Arshad\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Arshad\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Arshad\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Arshad\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Epoch 1/1\n",
      "8100/8100 [==============================] - 87s 11ms/step - loss: 0.6871 - accuracy: 0.5735\n"
     ]
    }
   ],
   "source": [
    "data = \"twitter\"\n",
    "model_type = \"blstm\"\n",
    "vector_type = \"random\"\n",
    "\n",
    "# Train and test the model\n",
    "model = run_model(data, 3, model_type, vector_type, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Document length : 63\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 63, 200)           1407600   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 63, 200)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 63, 400)           641600    \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 63, 400)           961600    \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 802       \n",
      "=================================================================\n",
      "Total params: 3,973,202\n",
      "Trainable params: 3,973,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from models import feature\n",
    "\n",
    "x_text,labels = get_data(\"twitter\",3)\n",
    "data_dict = get_train_test(data,  x_text, labels)\n",
    "input_value = data_dict[\"testX\"]\n",
    "\n",
    "# Get the intermediate output of the Bidirectional LSTM for a X_batch\n",
    "layer_idx = 4\n",
    "bilstm_output = feature(model, layer_idx, input_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
